SELECT COUNT(*) FROM SYS.EMPLOYEE WHERE EMPID=1
SELECT COUNT(*) FROM SYS.DEPART WHERE DEPT_ID=2

import os
import cx_Oracle
import pandas as pd
import subprocess
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("PPNR Accuracy Report") \
    .enableHiveSupport() \
    .getOrCreate()

def fetch_pcds_data():
    try:
        # Oracle connection details
        db_name = "CDS"
        user_var = f"USERNAME_{db_name}"
        passwd_var = f"PWD_{db_name}"
        service_var = f"CONNECT_STRING_{db_name}"
        user_name = os.environ[user_var]
        passwd = os.environ[passwd_var]
        service_nm = os.environ[service_var]

        # Parse the service string
        service_nm = service_nm[service_nm.index('@') + 1:]
        host = service_nm[0:service_nm.index(':')]
        port = service_nm[service_nm.index(':') + 1:service_nm.index('/')]
        service_nm = service_nm[service_nm.index('/') + 1:]

        # Create a DSN
        dsn = cx_Oracle.makedsn(host, port, service_name=service_nm)

        # Establish a connection
        connection = cx_Oracle.connect(user=user_name, password=passwd, dsn=dsn)
        cursor = connection.cursor()

        query = """
        SELECT extrnl_acct_id, past_due_151_180_amt
        FROM dmt_aspp_dba.asp_eom_acct_sum1
        WHERE ROWNUM <= 1000
        """

        cursor.execute(query)
        rows = cursor.fetchall()

        columns = [desc[0] for desc in cursor.description]
        data_df = pd.DataFrame(rows, columns=columns)

        cursor.close()
        connection.close()

        if data_df.empty:
            print("No data fetched from PCDS. Exiting...")
            raise ValueError("PCDS query returned no data.")

        # Validate columns
        required_columns = {"extrnl_acct_id", "past_due_151_180_amt"}
        if not required_columns.issubset(data_df.columns):
            print("Missing required columns in PCDS data. Exiting...")
            raise ValueError(f"Missing columns: {required_columns - set(data_df.columns)}")

        # Log success point
        print("PCDS data fetched successfully.")
        return data_df

    except Exception as e:
        print(f"Error while fetching data from PCDS: {e}")
        raise

def ensure_hive_query_output(output_file):
    try:
        # Hardcoded Hive command
        hive_command = """hive -e "INSERT OVERWRITE DIRECTORY 'hdfs://nameservice1/user/x01532741/hive_output' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT extrnl_acct_id, past_due_151_180_amt FROM usrf_in.cds_acct_c1;" """
        
        print(f"Executing Hive command: {hive_command}")
        subprocess.run(hive_command, shell=True, check=True)

        print(f"Hive query output written to: {output_file}")

    except subprocess.CalledProcessError as e:
        print(f"Error executing Hive query: {e}")
        raise

def fetch_and_compare_data(key_columns):
    try:
        # Fetch data from PCDS
        source_data = fetch_pcds_data()
        source_df = spark.createDataFrame(source_data)

        # Specify the HDFS path for Hive output
        output_file = "hdfs://nameservice1/user/x01532741/hive_output"

        # Ensure Hive query output is available
        ensure_hive_query_output(output_file)

        # Read Hive query result into a Spark DataFrame
        target_df = spark.read.csv(output_file, header=True, inferSchema=True)

        # Validate Hive DataFrame columns
        required_columns = {"extrnl_acct_id", "past_due_151_180_amt"}
        if not required_columns.issubset(target_df.columns):
            print("Missing required columns in Hive data. Exiting...")
            raise ValueError(f"Missing columns: {required_columns - set(target_df.columns)}")

        # Select 1000 accounts for comparison
        pcds_source_df = source_df.limit(1000)
        hive_target_df = target_df.limit(1000)

        # Compare data
        mismatches = pcds_source_df.join(hive_target_df, key_columns, how="outer") \
            .filter((pcds_source_df[key_columns[0]].isNull()) | (hive_target_df[key_columns[0]].isNull()) |
                    (pcds_source_df["past_due_151_180_amt"] != hive_target_df["past_due_151_180_amt"]))

        return mismatches

    except Exception as e:
        print(f"Error in fetch_and_compare_data: {e}")
        raise

def generate_accuracy_report(mismatches, total_records):
    try:
        mismatch_count = mismatches.count()
        accuracy = (1 - mismatch_count / total_records) * 100

        accuracy_report_path = "ppnr_accuracy_report.csv"
        with open(accuracy_report_path, "a") as report_file:
            report_file.write(f"Accuracy for PPNR: {accuracy:.2f}%\n")

        print(f"Accuracy for PPNR: {accuracy:.2f}% written to {accuracy_report_path}")

    except Exception as e:
        print(f"Error while generating accuracy report: {e}")
        raise

# Define source and target tables and attributes to compare
key_columns = ["extrnl_acct_id"]

try:
    mismatches = fetch_and_compare_data(key_columns)
    generate_accuracy_report(mismatches, 1000)
except Exception as e:
    print(f"Error during execution: {e}")

spark.stop()
