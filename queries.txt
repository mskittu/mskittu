SELECT COUNT(*) FROM SYS.EMPLOYEE WHERE EMPID=1
SELECT COUNT(*) FROM SYS.DEPART WHERE DEPT_ID=2

import os
import cx_Oracle
import pandas as pd
import subprocess
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("PPNR Accuracy Report") \
    .enableHiveSupport() \
    .getOrCreate()

def fetch_pcds_data(attributes, pcds_table, output_path):
    try:
        # Oracle connection details
        db_name = "CDS"
        user_var = f"USERNAME_{db_name}"
        passwd_var = f"PWD_{db_name}"
        service_var = f"CONNECT_STRING_{db_name}"
        user_name = os.environ[user_var]
        passwd = os.environ[passwd_var]
        service_nm = os.environ[service_var]

        # Parse the service string
        service_nm = service_nm[service_nm.index('@') + 1:]
        host = service_nm[0:service_nm.index(':')]
        port = service_nm[service_nm.index(':') + 1:service_nm.index('/')]
        service_nm = service_nm[service_nm.index('/') + 1:]

        # Create a DSN
        dsn = cx_Oracle.makedsn(host, port, service_name=service_nm)

        # Establish a connection
        connection = cx_Oracle.connect(user=user_name, password=passwd, dsn=dsn)
        cursor = connection.cursor()

        # Create a dynamic query for attributes
        attributes_str = ", ".join(attributes)
        query = f"""
        SELECT acct_bid AS ACCT_BID, {attributes_str}
        FROM {pcds_table}
        WHERE ROWNUM <= 100
        """
        cursor.execute(query)
        rows = cursor.fetchall()

        # Create DataFrame
        columns = [desc[0] for desc in cursor.description]
        data_df = pd.DataFrame(rows, columns=columns)

        cursor.close()
        connection.close()

        if data_df.empty:
            raise ValueError("PCDS query returned no data.")

        # Save PCDS data to CSV
        data_df.to_csv(output_path, index=False)
        print(f"Fetched {len(data_df)} records from PCDS and saved to {output_path}.")
        return data_df

    except Exception as e:
        print(f"Error while fetching data from PCDS: {e}")
        raise

def fetch_hive_data(attributes, hive_table, output_path):
    try:
        # Create dynamic Hive query
        attributes_str = ", ".join(attributes)
        hive_command = f"""
        hive -e "INSERT OVERWRITE LOCAL DIRECTORY '{output_path}' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
                 SELECT acct_bid, {attributes_str} 
                 FROM {hive_table}
                 LIMIT 100;"
        """
        subprocess.run(hive_command, shell=True, check=True)

        # Convert Hive output to CSV
        hive_data_path = os.path.join(output_path, "000000_0")
        hive_df = spark.read.csv(hive_data_path, header=False, inferSchema=True)
        hive_df.write.csv(output_path + ".csv", header=True, mode="overwrite")

        print(f"Hive data saved to {output_path}.csv")
        return hive_df

    except subprocess.CalledProcessError as e:
        print(f"Error executing Hive query: {e}")
        raise

def compare_data(pcds_data_path, hive_data_path, attributes):
    try:
        # Read PCDS data
        pcds_source_df = spark.read.csv(pcds_data_path, header=True, inferSchema=True)

        # Read Hive data
        hive_target_df = spark.read.csv(hive_data_path, header=True, inferSchema=True)

        # Validate columns
        required_columns = {"ACCT_BID", *attributes}
        if not required_columns.issubset(set(pcds_source_df.columns)):
            raise ValueError(f"PCDS data missing required columns: {required_columns - set(pcds_source_df.columns)}")
        if not required_columns.issubset(set(hive_target_df.columns)):
            raise ValueError(f"Hive data missing required columns: {required_columns - set(hive_target_df.columns)}")

        # Compare data
        conditions = [
            (pcds_source_df[attr] != hive_target_df[attr])
            for attr in attributes
        ]
        mismatches = pcds_source_df.join(hive_target_df, "ACCT_BID", how="outer") \
            .filter((pcds_source_df["ACCT_BID"].isNull()) | (hive_target_df["ACCT_BID"].isNull()) | any(conditions))

        return mismatches

    except Exception as e:
        print(f"Error during data comparison: {e}")
        raise

def generate_accuracy_report(mismatches, total_records, attributes, hive_table, pcds_table, final_report_path):
    try:
        mismatch_count = mismatches.count()
        accuracy = (1 - mismatch_count / total_records) * 100

        # Append results to the final accuracy report
        with open(final_report_path, "a") as report_file:
            report_file.write(f"Hive Table: {hive_table}, PCDS Table: {pcds_table}\n")
            report_file.write(f"Attributes: {', '.join(attributes)}\n")
            report_file.write(f"Accuracy: {accuracy:.2f}%\n")
            report_file.write(f"Total Records: {total_records}, Mismatches: {mismatch_count}\n\n")

        print(f"Accuracy report updated for {hive_table} vs {pcds_table}: {accuracy:.2f}%")
        return accuracy

    except Exception as e:
        print(f"Error generating accuracy report: {e}")
        raise

# Main Execution
attributes = ["FCFEE_CHRGOFF_RVRSL_AMT", "past_due_151_180_amt"]
tables = [
    {"hive_table": "cds_acct_c1", "pcds_table": "asp_eom_acct_sum"},
    {"hive_table": "cds_asp_eom_acct_sum", "pcds_table": "asp_eom_acct_sum"}
]

# Define paths
base_path = "/apps/tenant_local/usrf_stg/HadoopSandbox/x01532741/job/fin_recon"
pcds_output_path = os.path.join(base_path, "pcds_data.csv")
hive_output_path = os.path.join(base_path, "hive_data")
final_report_path = os.path.join(base_path, "ppnr_accuracy_report.csv")

# Clear existing accuracy report
open(final_report_path, "w").close()

try:
    for table in tables:
        # Fetch PCDS Data and Save
        fetch_pcds_data(attributes, table["pcds_table"], pcds_output_path)

        # Fetch Hive Data and Save
        fetch_hive_data(attributes, table["hive_table"], hive_output_path)

        # Compare Data
        mismatches = compare_data(pcds_output_path, hive_output_path + ".csv", attributes)

        # Generate Accuracy Report
        generate_accuracy_report(mismatches, 100, attributes, table["hive_table"], table["pcds_table"], final_report_path)

    print(f"Final PPNR Accuracy Report saved at {final_report_path}")

except Exception as e:
    print(f"Error during execution: {e}")

spark.stop()
