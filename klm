import os
import pandas as pd
import cx_Oracle
import subprocess

def fetch_pcds_data():
    try:
        output_path = "/app/tenant_local/usrf_stg/hadoop/name/barc/finance/sox_file/pcds_data.csv"
        
        # Oracle connection details
        db_name = "CDS"
        user_var = f"USERNAME_{db_name}"
        passwd_var = f"PWD_{db_name}"
        service_var = f"CONNECT_STRING_{db_name}"
        user_name = os.environ.get(user_var)
        passwd = os.environ.get(passwd_var)
        service_nm = os.environ.get(service_var)

        if not all([user_name, passwd, service_nm]):
            raise ValueError("Oracle environment variables are not set.")

        # Parse the service string
        service_nm = service_nm[service_nm.index('@') + 1:]
        host = service_nm[:service_nm.index(':')]
        port = service_nm[service_nm.index(':') + 1:service_nm.index('/')]
        service_nm = service_nm[service_nm.index('/') + 1:]

        # Create a DSN
        dsn = cx_Oracle.makedsn(host, port, service_name=service_nm)

        # Establish a connection
        connection = cx_Oracle.connect(user=user_name, password=passwd, dsn=dsn)
        cursor = connection.cursor()

        # PCDS Query
        pcds_query = """
        SELECT EXTNL_ACCT_ID, PAST_DUE_151_180_AMT
        FROM dmt_asp_dba.asp_eom_acct_sum
        WHERE ROWNUM <= 1000
        """
        cursor.execute(pcds_query)
        pcds_rows = cursor.fetchall()
        pcds_columns = [desc[0] for desc in cursor.description]
        pcds_df = pd.DataFrame(pcds_rows, columns=pcds_columns)

        cursor.close()
        connection.close()

        if pcds_df.empty:
            raise ValueError("PCDS query returned no data.")
        
        # Save to CSV
        pcds_df.to_csv(output_path, index=False)
        print(f"PCDS data saved to {output_path}")
        return pcds_df
    except Exception as e:
        print(f"Error while fetching data from PCDS: {e}")
        raise

def fetch_hive_intermediate_data():
    try:
        output_path = "/app/tenant_local/usrf_stg/hadoop/name/barc/finance/sox_file/hive_intermediate_data.csv"
        
        # Hive intermediate query
        hive_query = """
        SELECT EXTNL_ACCT_ID, INTERMEDIATE_COL1, INTERMEDIATE_COL2, INTERMEDIATE_COL3
        FROM usrf_ingest.hive_intermediate_table
        LIMIT 1000
        """

        # Execute Hive query and capture output
        hive_command = f"hive -e \"{hive_query}\""
        result = subprocess.run(hive_command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)

        # Process Hive query output
        lines = result.stdout.strip().split('\n')
        rows = [line.split('\t') for line in lines]

        if rows:
            columns = rows[0]
            data = rows[1:]
            hive_df = pd.DataFrame(data, columns=columns)
            
            # Clean invalid elements or attributes if necessary
            hive_df = hive_df.dropna()  # Removing any invalid/missing values
            
            # Save to CSV
            hive_df.to_csv(output_path, index=False)
            print(f"Hive Intermediate data saved to {output_path}")
            return hive_df
        else:
            raise ValueError("Hive intermediate query returned no data.")
    except subprocess.CalledProcessError as e:
        print(f"Error executing Hive intermediate query: {e.stderr}")
        raise

def merge_pcds_hive_intermediate():
    try:
        output_path = "/app/tenant_local/usrf_stg/hadoop/name/barc/finance/sox_file/final_merged_data.csv"
        
        pcds_df = fetch_pcds_data()
        hive_intermediate_df = fetch_hive_intermediate_data()

        # Merge PCDS and Hive Intermediate Data
        merged_df = pd.merge(pcds_df, hive_intermediate_df, on="EXTNL_ACCT_ID", how="outer")
        
        # Ensure the final merged dataframe has 5 columns
        expected_columns = ["EXTNL_ACCT_ID", "PAST_DUE_151_180_AMT", "INTERMEDIATE_COL1", "INTERMEDIATE_COL2", "INTERMEDIATE_COL3"]
        for col in expected_columns:
            if col not in merged_df.columns:
                merged_df[col] = None
        merged_df = merged_df[expected_columns]
        
        # Save merged data to final CSV
        merged_df.to_csv(output_path, index=False)
        
        print(f"Final merged PCDS and Hive Intermediate data saved to {output_path}")
        return output_path
    except Exception as e:
        print(f"Error merging and saving PCDS and Hive Intermediate data: {e}")
        raise

# Example usage
merge_pcds_hive_intermediate()
