import os
import pandas as pd
import cx_Oracle
import subprocess

def fetch_pcds_data(output_path):
    try:
        # Oracle connection details
        db_name = "CDS"
        user_var = f"USERNAME_{db_name}"
        passwd_var = f"PWD_{db_name}"
        service_var = f"CONNECT_STRING_{db_name}"
        user_name = os.environ.get(user_var)
        passwd = os.environ.get(passwd_var)
        service_nm = os.environ.get(service_var)

        if not all([user_name, passwd, service_nm]):
            raise ValueError("Oracle environment variables are not set.")

        # Parse the service string
        service_nm = service_nm[service_nm.index('@') + 1:]
        host = service_nm[:service_nm.index(':')]
        port = service_nm[service_nm.index(':') + 1:service_nm.index('/')]
        service_nm = service_nm[service_nm.index('/') + 1:]

        # Create a DSN
        dsn = cx_Oracle.makedsn(host, port, service_name=service_nm)

        # Establish a connection
        connection = cx_Oracle.connect(user=user_name, password=passwd, dsn=dsn)
        cursor = connection.cursor()

        # Hardcoded SQL query
        query = """
        SELECT EXTNL_ACCT_ID, PAST_DUE_151_180_AMT, ANOTHER_COLUMN
        FROM PCDS_TABLE_NAME
        WHERE ROWNUM <= 1000
        """
        cursor.execute(query)
        rows = cursor.fetchall()

        # Create DataFrame
        columns = [desc[0] for desc in cursor.description]
        data_df = pd.DataFrame(rows, columns=columns)

        cursor.close()
        connection.close()

        if data_df.empty:
            raise ValueError("PCDS query returned no data.")

        # Save PCDS data to the specified path
        data_df.to_csv(output_path, index=False)
        print(f"Fetched {len(data_df)} records from PCDS and saved to {output_path}")

        return data_df

    except Exception as e:
        print(f"Error while fetching data from PCDS: {e}")
        return None

def fetch_hive_data(output_path):
    try:
        # Hive query
        hive_query = """
        SELECT *
        FROM usrf_ingest.cds_acct_c
        LIMIT 1000
        """

        # Execute the Hive query
        hive_command = f"hive -S -e \"{hive_query}\""
        result = subprocess.run(
            hive_command,
            shell=True,
            check=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            universal_newlines=True
        )

        # Parse the query result into lines
        lines = result.stdout.strip().split('\n')

        if not lines:
            raise ValueError("Hive query returned no output.")

        # The first line contains the column headers
        hive_columns = lines[0].split('\t')
        print(f"Detected Hive Columns: {hive_columns}")

        # Remaining lines contain the data
        rows = []
        for i, line in enumerate(lines[1:], start=2):  # Start row count at 2 (header is line 1)
            row = line.split('\t')
            if len(row) != len(hive_columns):
                print(f"Skipping row {i}: {row} (expected {len(hive_columns)} columns, found {len(row)})")
                continue  # Skip rows with mismatched columns
            rows.append(row)

        # Ensure we have valid data
        if not rows:
            raise ValueError("No valid rows found in the Hive query output.")

        # Create a DataFrame from the parsed rows and columns
        hive_df = pd.DataFrame(rows, columns=hive_columns)

        # Save the DataFrame to the specified output path
        hive_df.to_csv(output_path, index=False)
        print(f"Hive data saved to {output_path}")

        return hive_df

    except subprocess.CalledProcessError as e:
        print(f"Error executing Hive query: {e.stderr}")
        return None

    except Exception as e:
        print(f"Error processing Hive output: {e}")
        return None

def clean_hive_data(hive_data_path):
    try:
        # Load the Hive data
        hive_data = pd.read_csv(hive_data_path, error_bad_lines=False, warn_bad_lines=True)

        # Remove rows containing invalid elements or attributes
        hive_data = hive_data[~hive_data.astype(str).apply(lambda x: x.str.contains("onMismatch", na=False)).any(axis=1)]

        # Save the cleaned Hive data back to the file
        hive_data.to_csv(hive_data_path, index=False)
        print(f"Cleaned Hive data saved to {hive_data_path}")

    except Exception as e:
        print(f"Error while cleaning Hive data: {e}")

def compare_and_generate_report(pcds_data_path, hive_data_path, report_path):
    try:
        # Read PCDS and Hive data directly using Pandas
        pcds_df = pd.read_csv(pcds_data_path)
        hive_df = pd.read_csv(hive_data_path)

        # Debug: Print column names before normalization
        print("Original PCDS Columns:", pcds_df.columns.tolist())
        print("Original Hive Columns:", hive_df.columns.tolist())

        # Normalize column names
        pcds_df.columns = pcds_df.columns.str.strip().str.upper()
        hive_df.columns = hive_df.columns.str.strip().str.upper()

        # Debug: Check normalized columns
        print("Normalized PCDS Columns:", pcds_df.columns.tolist())
        print("Normalized Hive Columns:", hive_df.columns.tolist())

        # Check for key column existence
        if "EXTNL_ACCT_ID" not in pcds_df.columns or "EXTNL_ACCT_ID" not in hive_df.columns:
            raise ValueError("Key column 'EXTNL_ACCT_ID' not found in both datasets after normalization.")

        # Ensure EXTNL_ACCT_ID is treated as a string
        pcds_df["EXTNL_ACCT_ID"] = pcds_df["EXTNL_ACCT_ID"].astype(str)
        hive_df["EXTNL_ACCT_ID"] = hive_df["EXTNL_ACCT_ID"].astype(str)

        # Find common columns
        common_columns = list(set(pcds_df.columns).intersection(hive_df.columns))
        common_columns = [col for col in common_columns if col != "EXTNL_ACCT_ID"]

        # Rename columns for merging
        pcds_renamed = {col: f"{col}_src" for col in common_columns}
        hive_renamed = {col: f"{col}_tgt" for col in common_columns}

        # Merge and compare data
        merged_df = pd.merge(
            pcds_df.rename(columns=pcds_renamed),
            hive_df.rename(columns=hive_renamed),
            on="EXTNL_ACCT_ID",
            how="outer"
        )

        # Add status column
        def determine_status(row):
            for column in common_columns:
                if row.get(f"{column}_src") != row.get(f"{column}_tgt"):
                    return "FAILED"
            return "PASS"

        merged_df["status"] = merged_df.apply(determine_status, axis=1)

        # Save the final report
        final_report_columns = ["EXTNL_ACCT_ID", "status"] + [
            f"{col}_src" for col in common_columns
        ] + [f"{col}_tgt" for col in common_columns]

        final_report_df = merged_df[final_report_columns].fillna("")
        final_report_df.columns = [col.lower() for col in final_report_df.columns]
        final_report_df.to_csv(report_path, index=False)
        print(f"Final PPNR Accuracy Report saved to {report_path}")

    except Exception as e:
        print(f"Error generating accuracy report: {e}")

# Main Execution
base_path = "/apps/tenant_local/usrf_stg/HadoopSandbox/x01532741/job/fin_recon"

try:
    # Fetch PCDS Data
    pcds_data_path = f"{base_path}/pcds_data.csv"
    pcds_data = fetch_pcds_data(pcds_data_path)

    # Fetch Hive Data
    hive_data_path = f"{base_path}/hive_data.csv"
    hive_data = fetch_hive_data(hive_data_path)

    # Clean Hive Data
    clean_hive_data(hive_data_path)

    # Compare Data and Generate Report
    compare_and_generate_report(
        pcds_data_path,
        hive_data_path,
        f"{base_path}/ppnr_accuracy_report.csv"
    )

except Exception as e:
    print(f"Error during execution: {e}")
