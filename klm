import os
import cx_Oracle
import pandas as pd
import subprocess
from pyspark.sql import SparkSession

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("PPNR Accuracy Report") \
    .getOrCreate()

def fetch_pcds_data(attributes, pcds_table, output_path):
    try:
        # Oracle connection details
        db_name = "CDS"
        user_var = f"USERNAME_{db_name}"
        passwd_var = f"PWD_{db_name}"
        service_var = f"CONNECT_STRING_{db_name}"
        user_name = os.environ.get(user_var)
        passwd = os.environ.get(passwd_var)
        service_nm = os.environ.get(service_var)

        if not all([user_name, passwd, service_nm]):
            raise ValueError("Oracle environment variables are not set.")

        # Parse the service string
        service_nm = service_nm[service_nm.index('@') + 1:]
        host = service_nm[0:service_nm.index(':')]
        port = service_nm[service_nm.index(':') + 1:service_nm.index('/')]
        service_nm = service_nm[service_nm.index('/') + 1:]

        # Create a DSN
        dsn = cx_Oracle.makedsn(host, port, service_name=service_nm)

        # Establish a connection
        connection = cx_Oracle.connect(user=user_name, password=passwd, dsn=dsn)
        cursor = connection.cursor()

        # Construct SQL query
        attributes_str = ", ".join(attributes)
        query = f"""
        SELECT acct_bid AS ACCT_BID, {attributes_str}
        FROM {pcds_table}
        WHERE ROWNUM <= 100
        """
        cursor.execute(query)
        rows = cursor.fetchall()

        # Create DataFrame
        columns = [desc[0] for desc in cursor.description]
        data_df = pd.DataFrame(rows, columns=columns)

        cursor.close()
        connection.close()

        if data_df.empty:
            raise ValueError("PCDS query returned no data.")

        # Save PCDS data to HDFS
        spark_df = spark.createDataFrame(data_df)
        spark_df.write.csv(output_path, header=True, mode="overwrite")
        print(f"Fetched {len(data_df)} records from PCDS and saved to HDFS at {output_path}")

        return data_df

    except Exception as e:
        print(f"Error while fetching data from PCDS: {e}")
        raise

def fetch_hive_data(output_path):
    try:
        # Hive query with LIMIT 100
        hive_query = """
        SELECT extrnl_acct_id, past_due_151_180_amt 
        FROM usrf_in.cds_acct_c
        LIMIT 100
        """

        # Execute Hive query and capture output
        hive_command = f"hive -e \"{hive_query}\""
        result = subprocess.run(hive_command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)

        # Split the query output into rows
        rows = [line.split('\t') for line in result.stdout.strip().split('\n')]

        # Save Hive data to HDFS
        if rows:
            hive_columns = ["EXTRNL_ACCT_ID", "PAST_DUE_151_180_AMT"]
            hive_df = pd.DataFrame(rows, columns=hive_columns)
            spark_df = spark.createDataFrame(hive_df)
            spark_df.write.csv(output_path, header=True, mode="overwrite")
            print(f"Hive data saved to HDFS at {output_path}")
            return hive_df
        else:
            raise ValueError("Hive query returned no data.")

    except subprocess.CalledProcessError as e:
        print(f"Error executing Hive query: {e.stderr}")
        raise

    except Exception as e:
        print(f"Error processing Hive output: {e}")
        raise

def compare_data(hdfs_path):
    try:
        # Paths for PCDS and Hive data on HDFS
        pcds_data_path = f"{hdfs_path}/pcds_data.csv"
        hive_data_path = f"{hdfs_path}/hive_data.csv"

        # Read PCDS data from HDFS
        pcds_source_df = spark.read.option("header", True).csv(pcds_data_path)

        # Read Hive data from HDFS
        hive_target_df = spark.read.option("header", True).csv(hive_data_path)

        # Validate columns
        required_columns_pcds = {"ACCT_BID", "PAST_DUE_151_180_AMT"}
        required_columns_hive = {"EXTRNL_ACCT_ID", "PAST_DUE_151_180_AMT"}
        if not required_columns_pcds.issubset(set(pcds_source_df.columns)):
            raise ValueError(f"PCDS data missing required columns: {required_columns_pcds - set(pcds_source_df.columns)}")
        if not required_columns_hive.issubset(set(hive_target_df.columns)):
            raise ValueError(f"Hive data missing required columns: {required_columns_hive - set(hive_target_df.columns)}")

        # Compare data
        mismatches = pcds_source_df.join(
            hive_target_df,
            pcds_source_df["ACCT_BID"] == hive_target_df["EXTRNL_ACCT_ID"],
            how="outer"
        ).filter(
            (pcds_source_df["PAST_DUE_151_180_AMT"] != hive_target_df["PAST_DUE_151_180_AMT"]) |
            (pcds_source_df["ACCT_BID"].isNull()) |
            (hive_target_df["EXTRNL_ACCT_ID"].isNull())
        )

        return mismatches

    except Exception as e:
        print(f"Error during data comparison: {e}")
        raise

def generate_accuracy_report(mismatches, total_records, output_path):
    try:
        mismatch_count = mismatches.count()
        accuracy = 100 if total_records == 0 else (1 - mismatch_count / total_records) * 100

        # Save report to HDFS
        accuracy_report = spark.createDataFrame(
            [(accuracy, total_records, mismatch_count)],
            schema=["Accuracy", "Total Records", "Mismatches"]
        )
        accuracy_report.write.csv(output_path, header=True, mode="overwrite")

        print(f"Accuracy report saved to HDFS at {output_path}")
        return accuracy

    except Exception as e:
        print(f"Error generating accuracy report: {e}")
        raise

# Main Execution
hdfs_path = "hdfs:/nameservice1/tmp/mds_accuracy_report"

try:
    # Fetch PCDS Data and Save to HDFS
    fetch_pcds_data(
        ["PAST_DUE_151_180_AMT"],
        "asp_eom_acct_sum",
        f"{hdfs_path}/pcds_data.csv"
    )

    # Fetch Hive Data and Save to HDFS
    fetch_hive_data(f"{hdfs_path}/hive_data.csv")

    # Compare Data
    mismatches = compare_data(hdfs_path)

    # Generate Accuracy Report
    generate_accuracy_report(mismatches, 100, f"{hdfs_path}/ppnr_accuracy_report.csv")

    print(f"Final PPNR Accuracy Report saved to HDFS at {hdfs_path}/ppnr_accuracy_report.csv")

except Exception as e:
    print(f"Error during execution: {e}")

spark.stop()
