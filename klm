import os
import pandas as pd
import cx_Oracle
import subprocess

# Function to fetch data from PCDS (Oracle Database)
def fetch_pcds_data():
    try:
        output_path = "/apps/tenant_local/usrf_stg/HadoopSandbox/x01532741/stage/barclays/finance/SOX_FILES/pcds_data.csv"

        # Fetch Oracle connection details from environment variables
        db_name = "CDS"
        user_var = f"USERNAME_{db_name}"
        passwd_var = f"PWD_{db_name}"
        service_var = f"CONNECT_STRING_{db_name}"
        user_name = os.environ.get(user_var)
        passwd = os.environ.get(passwd_var)
        service_nm = os.environ.get(service_var)

        # Validate if all required environment variables are set
        if not all([user_name, passwd, service_nm]):
            raise ValueError("Oracle environment variables are not set.")

        # Extract host, port, and service name from connection string
        service_nm = service_nm[service_nm.index('@') + 1:]
        host = service_nm[:service_nm.index(':')]
        port = service_nm[service_nm.index(':') + 1:service_nm.index('/')]
        service_nm = service_nm[service_nm.index('/') + 1:]

        # Create Oracle DSN and establish connection
        dsn = cx_Oracle.makedsn(host, port, service_name=service_nm)
        connection = cx_Oracle.connect(user=user_name, password=passwd, dsn=dsn)
        cursor = connection.cursor()

        # Query to fetch table counts from Oracle
        pcds_query = """
        SELECT 'acct_tbal_prcng_b_eoc_fact' AS table_name, COUNT(*) AS count
        FROM usrf_ingest.cds_acct_tbal_prcng_b_eoc_fact 
        WHERE date_column BETWEEN '20240101' AND '20240130'
        UNION ALL
        SELECT 'acct_tbal_prcng_c_eoc_fact' AS table_name, COUNT(*) AS count
        FROM usrf_ingest.cds_acct_tbal_prcng_c_eoc_fact 
        WHERE date_column BETWEEN '20240101' AND '20240130'
        """
        cursor.execute(pcds_query)
        pcds_rows = cursor.fetchall()
        pcds_columns = [desc[0] for desc in cursor.description]
        pcds_df = pd.DataFrame(pcds_rows, columns=pcds_columns)
        cursor.close()
        connection.close()

        # Validate if query returned data
        if pcds_df.empty:
            raise ValueError("PCDS query returned no data.")

        # Save fetched data to CSV
        pcds_df.to_csv(output_path, index=False)
        print(f"PCDS data saved to {output_path}")
        return pcds_df
    except Exception as e:
        print(f"Error while fetching data from PCDS: {e}")
        raise

# Function to fetch data from Hive Database
def fetch_hive_data():
    try:
        output_path = "/apps/tenant_local/usrf_stg/HadoopSandbox/x01532741/stage/barclays/finance/SOX_FILES/hive_data.csv"

        # Query to fetch table counts from Hive
        hive_query = """
        SELECT 'acct_tbal_prcng_b_eoc_fact' AS table_name, COUNT(*) AS count
        FROM usrf_ingest.cds_acct_tbal_prcng_b_eoc_fact 
        WHERE date_column BETWEEN '20240101' AND '20240130'
        UNION ALL
        SELECT 'acct_tbal_prcng_c_eoc_fact' AS table_name, COUNT(*) AS count
        FROM usrf_ingest.cds_acct_tbal_prcng_c_eoc_fact 
        WHERE date_column BETWEEN '20240101' AND '20240130'
        """
        
        # Execute Hive query via subprocess
        hive_command = f"hive -e \"{hive_query}\""
        result = subprocess.run(
            hive_command, shell=True, check=True,
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True
        )
        
        # Process Hive query output
        lines = result.stdout.strip().split('\n')
        hive_data = [line.split('\t') for line in lines]
        hive_df = pd.DataFrame(hive_data, columns=["table_name", "count"])
        hive_df["count"] = hive_df["count"].astype(int)
        
        # Save fetched data to CSV
        hive_df.to_csv(output_path, index=False)
        print(f"Hive data saved to {output_path}")
        
        # Clean Hive data before further processing
        clean_hive_data(output_path)
        return hive_df
    except Exception as e:
        print(f"Error fetching Hive data: {e}")
        raise

# Function to clean Hive data by removing invalid entries
def clean_hive_data(hive_data_path):
    try:
        hive_data = pd.read_csv(hive_data_path, error_bad_lines=False, warn_bad_lines=True)
        hive_data = hive_data[~hive_data.astype(str).apply(lambda x: x.str.contains("onMismatch", na=False)).any(axis=1)]
        hive_data.to_csv(hive_data_path, index=False)
        print(f"Cleaned Hive data saved to {hive_data_path}")
    except Exception as e:
        print(f"Error while cleaning Hive data: {e}")
        raise

# Function to compute variance between PCDS and Hive data
def compute_variance():
    try:
        pcds_df = fetch_pcds_data()
        hive_df = fetch_hive_data()
        
        # Merge PCDS and Hive data for comparison
        merged_df = pd.merge(pcds_df, hive_df, on="table_name", how="outer", suffixes=("_pcds", "_hive"))
        merged_df.fillna(0, inplace=True)
        merged_df["count_pcds"] = merged_df["count_pcds"].astype(int)
        merged_df["count_hive"] = merged_df["count_hive"].astype(int)
        
        # Compute variance and determine status
        merged_df["variance"] = merged_df["count_pcds"] - merged_df["count_hive"]
        merged_df["status"] = merged_df["variance"].apply(lambda x: "Match" if x == 0 else "Mismatch")
        
        # Save final comparison report
        output_path = "final_comparison_report.csv"
        merged_df.to_csv(output_path, index=False)
        print(f"Comparison report generated at {output_path}")
        return output_path
    except Exception as e:
        print(f"Error computing variance: {e}")
        raise

# Entry point to execute the script
if __name__ == "__main__":
    compute_variance()
