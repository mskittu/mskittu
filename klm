#!/bin/bash

# Define required variables
LAST_DATE_PREV_MONTH=20250228
Local_DeliveryFilePath="/path/to/local/directory"  # Replace with actual path
Hdfs_DeliveryFilePath="/path/to/hdfs/directory"  # Replace with actual HDFS path

# Remove old files from HDFS (excluding DMA_CR_BURAU_TRU)
hdfs dfs -ls ${Hdfs_DeliveryFilePath}/ | awk '{print $8}' | grep "${LAST_DATE_PREV_MONTH}" | grep -v "DMA_CR_BURAU_TRU" | xargs hdfs dfs -rm -skipTrash

# Check if files already exist in HDFS
hdfs dfs -test -e ${Hdfs_DeliveryFilePath}/DMA_CR_BURAU_TRU_C_${LAST_DATE_PREV_MONTH}.csv
file1_exists=$?
hdfs dfs -test -e ${Hdfs_DeliveryFilePath}/DMA_CR_BURAU_TRU_B_${LAST_DATE_PREV_MONTH}.csv
file2_exists=$?

# If files do not exist, copy from local to HDFS
if [[ $file1_exists -ne 0 || $file2_exists -ne 0 ]]; then
    echo "Copying files to HDFS..."
    hadoop fs -copyFromLocal -f ${Local_DeliveryFilePath}/*${LAST_DATE_PREV_MONTH}*.csv ${Hdfs_DeliveryFilePath}/
else
    echo "Files already exist in HDFS, skipping copy."
fi
