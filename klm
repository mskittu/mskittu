import os
import pandas as pd
import cx_Oracle
import subprocess


def fetch_pcds_data(attributes, pcds_table, output_path):
    try:
        # Oracle connection details
        db_name = "CDS"
        user_var = f"USERNAME_{db_name}"
        passwd_var = f"PWD_{db_name}"
        service_var = f"CONNECT_STRING_{db_name}"
        user_name = os.environ.get(user_var)
        passwd = os.environ.get(passwd_var)
        service_nm = os.environ.get(service_var)

        if not all([user_name, passwd, service_nm]):
            raise ValueError("Oracle environment variables are not set.")

        # Parse the service string
        service_nm = service_nm[service_nm.index('@') + 1:]
        host = service_nm[0:service_nm.index(':')]
        port = service_nm[service_nm.index(':') + 1:service_nm.index('/')]
        service_nm = service_nm[service_nm.index('/') + 1:]

        # Create a DSN
        dsn = cx_Oracle.makedsn(host, port, service_name=service_nm)

        # Establish a connection
        connection = cx_Oracle.connect(user=user_name, password=passwd, dsn=dsn)
        cursor = connection.cursor()

        # Construct SQL query
        attributes_str = ", ".join(attributes)
        query = f"""
        SELECT acct_bid AS ACCT_BID, {attributes_str}
        FROM {pcds_table}
        WHERE ROWNUM <= 100
        """
        cursor.execute(query)
        rows = cursor.fetchall()

        # Create DataFrame
        columns = [desc[0] for desc in cursor.description]
        data_df = pd.DataFrame(rows, columns=columns)

        cursor.close()
        connection.close()

        if data_df.empty:
            raise ValueError("PCDS query returned no data.")

        # Save PCDS data to the specified path
        data_df.to_csv(output_path, index=False)
        print(f"Fetched {len(data_df)} records from PCDS and saved to {output_path}")

        return data_df

    except Exception as e:
        print(f"Error while fetching data from PCDS: {e}")
        raise


def fetch_hive_data(output_path):
    try:
        # Hive query with LIMIT 100
        hive_query = """
        SELECT extrnl_acct_id, past_due_151_180_amt 
        FROM usrf_ingest.cds_acct_c
        LIMIT 100
        """

        # Execute Hive query and capture output
        hive_command = f"hive -e \"{hive_query}\""
        result = subprocess.run(hive_command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)

        # Split the query output into rows
        rows = [line.split('\t') for line in result.stdout.strip().split('\n')]

        # Save Hive data to the specified path
        if rows:
            hive_columns = ["EXTRNL_ACCT_ID", "PAST_DUE_151_180_AMT"]
            hive_df = pd.DataFrame(rows, columns=hive_columns)
            hive_df.to_csv(output_path, index=False)
            print(f"Hive data saved to {output_path}")
            return hive_df
        else:
            raise ValueError("Hive query returned no data.")

    except subprocess.CalledProcessError as e:
        print(f"Error executing Hive query: {e.stderr}")
        raise

    except Exception as e:
        print(f"Error processing Hive output: {e}")
        raise


def clean_hive_data(hive_data_path):
    try:
        # Load the Hive data
        hive_data = pd.read_csv(hive_data_path, error_bad_lines=False, warn_bad_lines=True)

        # Remove rows containing invalid elements or attributes
        hive_data = hive_data[~hive_data.astype(str).apply(lambda x: x.str.contains("onMismatch", na=False)).any(axis=1)]

        # Save the cleaned Hive data back to the file
        hive_data.to_csv(hive_data_path, index=False)
        print(f"Cleaned Hive data saved to {hive_data_path}")

    except Exception as e:
        print(f"Error while cleaning Hive data: {e}")
        raise


def compare_and_generate_report(pcds_data_path, hive_data_path, report_path, column_mappings):
    try:
        # Read PCDS and Hive data directly using Pandas
        pcds_df = pd.read_csv(pcds_data_path)
        hive_df = pd.read_csv(hive_data_path)

        # Ensure keys have the same data type
        pcds_df["ACCT_BID"] = pcds_df["ACCT_BID"].astype(str)
        hive_df["EXTRNL_ACCT_ID"] = hive_df["EXTRNL_ACCT_ID"].astype(str)

        # Rename columns based on mappings
        for src_column, tgt_column in column_mappings:
            pcds_df = pcds_df.rename(columns={src_column: f"{src_column}_src"})
            hive_df = hive_df.rename(columns={tgt_column: f"{tgt_column}_tgt"})

        # Merge and compare data
        merged_df = pd.merge(
            pcds_df,
            hive_df,
            left_on="ACCT_BID", right_on="EXTRNL_ACCT_ID",
            how="outer"
        )

        # Handle mismatched or missing values
        for src_column, tgt_column in column_mappings:
            src_col_prefixed = f"{src_column}_src"
            tgt_col_prefixed = f"{tgt_column}_tgt"
            merged_df[f"{src_column}_status"] = merged_df.apply(
                lambda row: "PASS" if row[src_col_prefixed] == row[tgt_col_prefixed] else "FAILED",
                axis=1
            )

        # Save the final report
        selected_columns = ["EXTRNL_ACCT_ID"] + [
            col for mapping in column_mappings for col in [f"{mapping[0]}_status", f"{mapping[0]}_src", f"{mapping[1]}_tgt"]
        ]
        final_report_df = merged_df[selected_columns].fillna("")
        final_report_df.to_csv(report_path, index=False)
        print(f"Final PPNR Accuracy Report saved to {report_path}")

    except Exception as e:
        print(f"Error generating accuracy report: {e}")
        raise


# Main Execution
base_path = "/apps/tenant_local/usrf_stg/HadoopSandbox/x01532741/job/fin_recon"

try:
    # Fetch PCDS Data and Save to Local Path
    fetch_pcds_data(
        ["PAST_DUE_151_180_AMT"],
        "asp_eom_acct_sum",
        f"{base_path}/pcds_data.csv"
    )

    # Fetch Hive Data and Save to Local Path
    fetch_hive_data(f"{base_path}/hive_data.csv")

    # Clean Hive Data
    clean_hive_data(f"{base_path}/hive_data.csv")

    # Define column mappings (source to target)
    column_mappings = [
        ("PAST_DUE_151_180_AMT", "PAST_DUE_151_180_AMT"),
        # Add more mappings here if needed
    ]

    # Compare Data and Generate Report
    compare_and_generate_report(
        f"{base_path}/pcds_data.csv",
        f"{base_path}/hive_data.csv",
        f"{base_path}/ppnr_accuracy_report.csv",
        column_mappings
    )

    print(f"Final PPNR Accuracy Report saved to {base_path}/ppnr_accuracy_report.csv")

except Exception as e:
    print(f"Error during execution: {e}")
